
# Analyse and compare PDEPA


This folder contains seven bash and python3 scripts that automatise the task of running benchs on the `pdepa` tool.
Since `pdepa` is a dependency solver for gentoo packages, a *test* is a list of packages that is asked to be installed on the current architecture.
A bench thus consists of asking `pdepa` -- and possibly other tools that we want to compare to `pdepa`, like `emerge` -- to solve many tests, possibly several times.
We then check the given answer each tool gave, together with their time and memory consumption.

A bench testing `pdepa` and `emerge` on 10000 tests, run twice, can be obtained with the following sequence of command line
1. `python bench_gen.py 10000 1 20> list.txt`: this command line generates a list of 10000 random tests that ask between 1 and 20 packages to be installed and store that list in the file `list.txt`
2. `bash bench_send.sh -d remote.txt list.txt`: this command line distributes the 10000 tests on the cloud architecture specified in the `remote.txt` file. The format of this file will be described later.
3. `bash bench_run.sh -l list.txt -no standard bench_1 bench_2`: this command line, run on every remote node of the cloud, calls `emerge` and `pdepa` on each test in the file `list.txt` once for bench directory in parameter (here, `bench_1` and `bench_2`). It stores the output of these tools, as well as their time and memory consumption in a file named `bench_dir/test_name/tool.out`. Note the option `-no standard` that desactivates running the standard feature analysis approach on the tests (implemented with the `standard.py` script)
4. `bash bench_data.sh bench_1 bench_2`: this command line, run on every remote node of the cloud, analyses the data generated by the tool in each of the bench directory in parameter (here, `bench_1` and `bench_2`), and generates a file CSV file `bench_dir/table.csv` where each row corresponds to a test and gives for each tool if it failed to find a solution for that test, how much time and how much memory it took. It also stores how many "features" `pdepa` loaded, compared to how many are declared in the gentoo repository
5. `bash bench_wget.sh -d remote.txt bench_1 bench_2`: this command line retrieves and combine the `table.csv` files from the remote nodes of the cloud, from each bench directory in parameter. In this case, this would generate two files, `bench_1/table.csv` that combines all `bench_1/table.csv` files on the cloud nodes, and `bench_2/table.csv` that combines all `bench_2/table.csv` files on the cloud nodes
6. `python bench_data.py bench_1 bench_2` computes the mean value for each test (in this case, we only have two values per test) and generates several files:
  * `table.csv` that stores the mean value for each test (this file follows the same format of the `bench_*/table.csv` files)
  * `bench_stds.csv` that stores the min, max and average standard deviation for each analysed value of each test
  * `order.txt` that stores the order in which the tests are displayed in the following images (sorted by the number of features loaded by `pdepa` during its solving process)
  * feature_loaded_pct.svg: a graphic showing the precentage of feature loaded by `pdepa` compared to the total number of features, with its min, mean and max value
  * emerge_time.svg:        a graphic showing the time `emerge` took to solve each test, with its min, mean and max value
  * pdepa_time.svg:         a graphic showing the time `pdepa` took to solve each test, with its min, mean and max value
  * standard_time.svg:      a graphic showing the time the standard approach took to solve each test, mean and max value
  * emerge_memory.svg:  a graphic showing the memory `emerge` took to solve each test, mean and max value
  * pdepa_memory.svg:  a graphic showing the memory `pdepa` took to solve each test, with its min, mean and max value
  * standard_memory.svg:  a graphic showing the memory the standard approach took to solve each test, mean and max value
  * failure.svg:  a bar plot showing for each test and each tool if that tool failed for that test or not. This plot is not very readable with high number of tests.

Note that the the two scripts `bench_gen.py` and `bench_run.sh` must be run in a environment in which portage (with a correctly configured repository) is available.
It is however possible to run these scripts using docker to avoid this restriction.
Currently, two docker images are available: `gzoumix/pdepa:latest` contains the latest version of `pdepa` within a gentoo distribution, and `gzoumix/pdepa:icse2020` contains the version of `pdepa` and gentoo that was used to produce the results published [here]() in the conference [ICSE'20](https://conf.researchr.org/home/icse-2020).
Using the docker image of your choice (that we name `vm_image` in this example), command lines 1 and 3 become:
1. `docker run vm_image bash -c "python /opt/pdepa/src/test/bench/bench_gen.py 10000 1 20" > list.txt`
3. `bash bench_run.sh -k vm_image -l list.txt -no standard bench_1 bench_2`


In the rest of this document, we will first talk about the format of the files used in these benchs, and  document more in depth the different scripts in this folder.


### File Formats

#### list.txt

This file contains one test per line.
A test starts with the name of the test, and follows with thelist of packages to install, separated with a space.
For instance the following file contains four tests, named `test_0`, `test_1`, `test_2` and `test_3`, and require the installation of between 1 and 7 packages
```
test_0 sec-policy/selinux-brctl sec-policy/selinux-munin media-sound/terminatorx sys-apps/baselayout media-libs/ptex dev-libs/libvterm
test_1 app-dicts/sword-chiun dev-libs/uthash dev-perl/Version-Next dev-haskell/regex-applicative-text app-dicts/stardict-cedict-zh-en-big5 dev-ruby/ruby-gnome2
test_2 app-misc/boxes dev-haskell/quickcheck-instances net-libs/biblesync dev-lang/bff media-libs/libtheora virtual/perl-Locale-Maketext-Simple games-action/maelstrom
test_3 games-util/lutris
```

#### remote.txt

This file contains the connexion information (with the ssh/scp syntax) of each remote node, one per line.
For instance, the following file lists two connexion nodes, one on localhost and one on a specific IP address
```
user@localhost
distant_user@8.8.8.8
```

#### table.csv

This file follows the csv standard, with the exception that columns are separated with spaces.
Like we previously discussed, each row of this file correspond to a test.
The columns of this file are as follows:
 * TEST: the name of the test
 * emerge_time: the computation time of `emerge`
 * emerge_memory: the memory consumption of `emerge`
 * emerge_success: True/False that says if `emerge` found a solution for this test or not
 * pdepa_time: the computation time of `pdepa`
 * pdepa_memory: the memory consumption of `pdepa`
 * pdepa_success: True/False that says if `pdepa` found a solution for this test or not
 * standard_time: the computation time of the standard approach
 * standard_memory: the memory consumption of the standard approach
 * standard_success: True/False that says if the standard approach found a solution for this test or not
 * feature_full: the number of feature in the portage repository
 * feature_loaded: the number of features loaded by pdepa

Note that if the execution of a tool is disabled (with the `-no` option), the table will still contain the column related to this tool, except that these column will always say that the tool failed in 0 second and consuming 0Mb of memory.


#### bench_stds.csv

This csv file (with spaces as column separator) gives meta statistics over the test repetition.
This file has 11 rows:
 * 9 for the time and memory consumption of each tool plus their success status
 * one for the `feature_loaded` column 
 * and one for the percentage of the number of loaded features
The columns give for every of these information:
 * the minimum standard deviation of that value for a test repetition.
 * the maximum standard deviation of that value for a test repetition.
 * the standard defition of that value, combining all tests.

For instance, let's consider the value `emerge_time` on three tests repeated 5 times, which give the following emerge execution time:
```
test_0      7.64     7.18     8.16     7.71     7.23
test_1      3.69     3.03     3.56     3.30     3.38
test_2      1.41     1.15     1.23     1.21     1.29
```
These execution time give a standard deviation of `0.399787` for `test_0`, `0.253318` for `test_1` and `0.098590` for `test_2`.
Hence, the row `emerge_time` in the `bench_stds.csv` file is:
```
emerge_time 0.098590 0.399787 0.27906531441223575
```



#### order.txt

This file lists every test name, one per line, in the order in which they are displayed in the graphs.




### Scripts documentation

In the following, we describe in more detail each scripts in this folder.

#### bench_gen.py
This script randomly generates tests from the available portage repository.
More precisely, it hooks into the portage API, get the packages that portage can discover in the repository it can access, and randomly selects packages (without considering the version) from this list.

###### Options
```
usage: bench_gen.py [-h] nb_test min_length max_length

positional arguments:
  nb_test     number of test to perform
  min_length  min length of the list
  max_length  max length of the list

optional arguments:
  -h, --help  show this help message and exit
```

#### bench_send.sh
This script uniformly distributes a test list file, and send the test to the remote nodes in a file with the same name as the input file.
Additionally, with the `-s` option, this script also send the `bench_run.sh` and `bench_data.sh` scripts, that need to be executed on the remote nodes.

#### Options
```
bench_send.sh -h|--help
bench_send.sh [-d DISTRIB_FILE] [-s] TEST_FILE
     -h|--help        print this message
     -d DISTRIB_FILE  set the distribution where to get the bench data from. If not given, reads from the standard input.
     -s               also send the bench_run.sh script to the cluster
     TEST_FILE        the file containing all the tests to perform
```

#### bench_run.sh
This script run the test, using a subset of preselected tools: currently, `pdepa`, `emerge` and `standard` are available.
Tools from that list can be unselected with the `-no` option.
To run the scripts, the user must give in parameter the folder in which the results of the tests will be saved: giving several folder means that the test will be run several time, one per folder.

#### Options
```
bench_run.sh -h|--help
bench_run.sh [-l LIST_FILE] [-c CONCUR] [-k DOCKER_IMAGE] [-no pdepa|emerge|standard] BENCHDIR+
     -h|--help                    print this message
     -l LIST_FILE                 the file containing the list of tests to perform. If none given, read from the standard input
     -c CONCUR                    sets the number of concurrent tests
     -k DOCKER_IMAGE              sets the docker image to use
     -no emerge|pdepa|standard    do not run the test for emerge, pdepa or the standard
     BENCHDIR                     sets the directory where to store the benchs
```


#### bench_data.sh
This script analyses the outputs produced in every test to generate a corresponding `table.csv` file at the root of the bench directory.
Like for the `bench_run.sh` script, it is possible to specify several bench folder for this script, which will generate a `table.csv` for every folder.

#### Options
```
bench_data.sh -h|--help
bench_data.sh BENCHDIR+
     -h|--help        print this message
     BENCHDIR         the directory where the benchs are stored
```


#### bench_wget.sh
This script collects and combines the `table.csv` generated on the remote nodes for each run of the tests.
For instance, considering that the tests were run in the bench folders `bench_1` and `bench_2` on each remote location, the files `bench_1/table.csv` and `bench_2/table.csv` only contain data on the tests run locally on that remote note. Running this script will create two files `bench_1/table.csv` and `bench_2/table.csv` that will respectively contain the content of the files `bench_1/table.csv` (resp. `bench_2/table.csv`) of all remote nodes, thus containing the data of all tests.

#### Options
```
bench_wget.sh -h|--help
bench_wget.sh [-d DISTRIB_FILE] [--prefix PREFIX] BENCHDIR+
     -h|--help        print this message
     -d DISTRIB_FILE  set the distribution where to get the bench data from. If not given, reads from the standard input.
     --prefix PREFIX  set the prefix in which the tables will be downloaded.
     BENCHDIR         the directories where the benchs are remotely stored.
```

### bench_data.py
This scripts generates several files and graphs that combine the results of all runs of all tests.

#### Options
```
usage: bench_data.py [-h] [-d DIR] [-n NB_XTICKS] benchdir [benchdir ...]

positional arguments:
  benchdir                                the directories where to find the bench data in table.csv

optional arguments:
  -h, --help                              show this help message and exit
  -d DIR, --dir DIR                       the directory in which to store the statistics
  -n NB_XTICKS, --nb_xticks NB_XTICKS     the number of x ticks in the generated graphs
```

### standard.py 
This script has several usage.
Its main usage is to implement the standard approach to analyse a feature model: first load the complete feature model, and then analyse it.
This is done using the `check` functionality of the tool, which takes the same parameters as `pdepa`.

Its second usage is to give how many feature are present in the current portage repository.
This is done using the `features` functionality of the tool, which takes the same parameters as `pdepa` without the list of packages to install.
Indeed, the number of features depends on which package are considered, as well as which use flags can be configured.

Finally, its last usage is to translate the portage repository into a CNF constraint that can be given in input to any SAT solver or SAT counter (that counts how many solution a SAT problem has).
This is done using the `cnf` functionality of the tool, wich, like the `features` functionality, takes the same parameters as `pdepa` without the list of packages to install.

#### Options
```
usage: standard.py check [-h] [-U [{all,+,-}]] [-M [{all,keyword,mask}]] [-C [{all,newuse}]] [-v] [-- SEP] [-d] package [package ...]

positional arguments:
  package               the cp to install

optional arguments:
  -h, --help                                                       show this help message and exit
  -U [{all,+,-}], --explore-use [{all,+,-}]                        allow the tool to set use flags
  -M [{all,keyword,mask}], --explore-mask [{all,keyword,mask}]     allow the tool to unmask packages
  -C [{all,newuse}], --explore-installed [{all,newuse}]            allow the tool to also consider the installed packages in its dependency computation
  -v, --verbose                                                    increase tool verbosity
  -- SEP                                                           separator between the option and the list of packages
  -d                                                               load only the dependencies of the cp to install, instead of the whole feature model
```
```
usage: standard.py {features,cnf} [-h] [-U [{all,+,-}]] [-M [{all,keyword,mask}]] [-C [{all,newuse}]] [-v] [-- SEP]

optional arguments:
  -h, --help                                                       show this help message and exit
  -U [{all,+,-}], --explore-use [{all,+,-}]                        allow the tool to set use flags
  -M [{all,keyword,mask}], --explore-mask [{all,keyword,mask}]     allow the tool to unmask packages
  -C [{all,newuse}], --explore-installed [{all,newuse}]            allow the tool to also consider the installed packages in its dependency computation
  -v, --verbose                                                    increase tool verbosity
  -- SEP                                                           separator between the option and the list of packages
```


